A. The Training Process (Offline)When: This happens in train_offline.py before the live trading day begins.The Input (Replay Simulation): The script loads historical Parquet files (fetched via REST) and "replays" them. It feeds them into OrderbookState and TickStore exactly as if they were coming from a live WebSocket. This ensures the model is trained on the exact same data structures it will see in production.The Target (The "Teacher"):Since there is no "god view" of the true probability, the system uses the midpoint price as a proxy for the truth.It constructs a Gaussian Target ($Q_k$) centered around the historical midpoint ($p_{mid}$).Logic: The model is trained to say, "Given this messy orderbook state, predict a smooth probability distribution centered on where the price actually ended up."B. The Learned Artifact (The Model)The Architecture: It is a ProbDistributionMLP (Multi-Layer Perceptron).What it learns: It learns a mapping function $f(x) \rightarrow P$:Input ($x$): A 9-dimensional vector including spread, depth imbalance, quote velocity, and the clever "rating diff" prior.Output ($P$): A probability distribution over a grid (e.g., probabilities for outcomes at 5¢, 10¢... 95¢).Why this matters: The model isn't just learning to predict a price; it's learning to predict uncertainty. If the history showed that high volatility leads to erratic prices, the model learns to output a wider, flatter curve (higher variance) rather than a sharp peak.C. The Live Execution (Online)When: This happens in pipeline.py.Logic: The system loads the frozen weights (.pt file) saved during offline training. It feeds live WebSocket data into this frozen model to generate real-time probability distributions.# aster
